# Proyecto 3 - MLOps (Fase Local)

Este proyecto implementa una arquitectura bÃ¡sica de MLOps de manera local para el procesamiento de datos, entrenamiento y predicciÃ³n de un modelo de clasificaciÃ³n, utilizando FastAPI y almacenamiento en MySQL.

---

## ğŸ“ Estructura del Proyecto

```text
Proyecto_3/
â”‚
â”œâ”€â”€ app_back/                      # Backend con FastAPI
â”‚   â””â”€â”€ main.py                    # Endpoints de carga, procesamiento, entrenamiento y predicciÃ³n
â”‚
â”œâ”€â”€ app_front/                     # Interfaz Streamlit (opcional)
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ connections/                   # ConfiguraciÃ³n de conexiÃ³n a MySQL (RAW y CLEAN)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ mysql_connections.py
â”‚
â”œâ”€â”€ dags/                          # Scripts de procesamiento, carga y entrenamiento
â”‚   â”œâ”€â”€ carga_datos.py             # Inserta datos crudos en RAW
â”‚   â”œâ”€â”€ preprocesar_datos.py       # Procesa datos y los guarda por lotes en CLEAN
â”‚   â””â”€â”€ train_model.py             # Entrena modelos por lote, guarda mÃ©tricas y el mejor modelo
â”‚
â”œâ”€â”€ data/Diabetes/                 # Dataset original
â”‚   â””â”€â”€ Diabetes.csv
â”‚
â”œâ”€â”€ models/                        # Modelos serializados entrenados
â”‚   â”œâ”€â”€ modelo_entrenado_batch1.pkl
â”‚   â”œâ”€â”€ modelo_entrenado_batch2.pkl
â”‚   â”œâ”€â”€ modelo_entrenado_batch3.pkl
â”‚   â”œâ”€â”€ modelo_entrenado_batch4.pkl
â”‚   â”œâ”€â”€ modelo_entrenado_batch5.pkl
â”‚   â”œâ”€â”€ modelo_entrenado_batch6.pkl
â”‚   â””â”€â”€ modelo_entrenado_final.pkl  # Mejor modelo basado
â”‚
â”œâ”€â”€ imagenes/                      # Recursos grÃ¡ficos
â”‚   â”œâ”€â”€ arquitectura.png
â”‚   â””â”€â”€ Front_Streamlit.png
â”‚
â”œâ”€â”€ requirements.txt               # Dependencias del proyecto
â””â”€â”€ README.md                      # DocumentaciÃ³n del proyecto
```

## âœ… Funcionalidad Actual


- **Carga de datos**  
  Inserta datos crudos en la tabla `initial_data` del esquema `raw_data`.

- **Preprocesamiento**  
  Limpia los datos, elimina columnas irrelevantes, transforma variables categÃ³ricas y divide el dataset en conjuntos de `entrenamiento`, `validaciÃ³n` y `prueba`. El conjunto `train` se mezcla aleatoriamente y se guarda por lotes en `clean_data.train_data`.

- **Entrenamiento del modelo**  
  Entrena mÃºltiples modelos `RandomForestClassifier` (uno por cada batch) usando `Pipeline` de `scikit-learn`, guarda cada uno como `modelo_entrenado_batch{n}.pkl`, registra sus mÃ©tricas (`accuracy`, `precision`, `recall`, `f1-score`) en la tabla `clean_data.experiments` y selecciona el mejor modelo basado en `val_f1`, copiÃ¡ndolo como `modelo_entrenado_final.pkl`.

- **PredicciÃ³n vÃ­a API**  
  Utiliza el modelo `modelo_entrenado_final.pkl` para predecir nuevas muestras y guarda las predicciones con sus probabilidades en `raw_data.predictions`.

- **Interfaz grÃ¡fica con Streamlit**  
  Permite a los usuarios realizar predicciones desde un formulario web amigable, que se comunica con la API FastAPI para enviar datos y mostrar resultados.

  ![Interfaz Streamlit](imagenes/Front_Streamlit.png)

- **API REST con FastAPI**  
  Permite ejecutar cada etapa del flujo mediante endpoints:

| Endpoint        | AcciÃ³n                                                   |
|----------------|----------------------------------------------------------|
| `/download`     | Descarga el dataset e inserta en `raw_data.initial_data` |
| `/clean`        | Preprocesa y divide los datos, luego guarda por lotes    |
| `/train`        | Entrena todos los modelos por lote y selecciona el mejor |
| `/predict`      | Realiza predicciones usando el mejor modelo entrenado    |

---

## ğŸ§° Requisitos y Puesta en Marcha

Sigue estos pasos para ejecutar el proyecto localmente:

```bash
# 1. Clonar el repositorio
git clone git@github.com:JohnSanchez27/MLOPS_Proyectos_3.git
cd MLOPS_Proyectos_3

# 2. Crear y activar un entorno virtual
# En Windows:
python -m venv .venv
.venv\Scripts\activate
# En Linux / macOS:
python3 -m venv .venv
source .venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Asegurar que el servidor MySQL estÃ© activo y con los esquemas raw_data y clean_data creados
#    Editar el archivo de conexiÃ³n: connections/mysql_connections.py

# 5. Iniciar el backend (FastAPI)
uvicorn app_back.main:app --reload
# Accede a la documentaciÃ³n: http://localhost:8000/docs

# 6. Iniciar la interfaz grÃ¡fica (Streamlit)
streamlit run app_front/app.py
